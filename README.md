# TUNE_Benchmark
Human-Centered Exploration of Table Unionability

This repository contains the official benchmark materials, data, experiment code, results, and technical report for the study “Human-Centered Exploration of Table Unionability.”
TUNE is the first benchmark that integrates human behavioral signals, automated TUS method outputs, and machine-learning models into a unified evaluation framework for studying unionability decisions.

The repository is organized to support full reproducibility, transparent data documentation, and modular experimentation.


## 1. Repository Structure

```
TUNE_Benchmark/
│
├── code/                     # Experiment scripts for Scenarios 1–4
│   ├── README.md             # Instructions for running ML experiments
│   ├── s1_experiment.py
│   ├── s2_experiment.py
│   ├── s3_experiment.py
│   └── s4_experiment.py
│
├── data/                     # Benchmark datasets and metadata
│   ├── README.md             # Data descriptions and schemas
│   ├── Compiled_Version.xlsx
│   ├── Compiled_Version_TUS.xlsx
│   ├── Feature_Engineered.csv
│   ├── Qualtrics export.csv
│   ├── RS_Compiled Version.xlsx
│   └── Survey_Tables.xlsx
│
├── results/                  # Outputs generated by the experiment scripts
│   ├── README.md             # Explanation of metrics and ablations
│   └── ablation study results.xlsx
│
├── Technical_Report.pdf      # Extended benchmark documentation and analysis
├── LICENSE                   # MIT license
└── README.md                 # (You are here) high-level overview
```

## 2. What Is TUNE?

TUNE (Table Unionability Evaluation) is a benchmark designed to answer a foundational research question:

How do humans, automated TUS methods, and machine-learning models reason about table unionability—and what happens when we combine them?

# TUNE uniquely brings together:

Human Behavioral Data

Unionability judgments

Confidence scores

Decision time

Free-text explanations

Multiple survey versions

Behavioral traces usable as ML features

Automated TUS Method Scores: Starmie, SANTOS, D3L. 

Scores from these methods are provided in Compiled_Version_TUS.xlsx.

# ML Models Across Four Scenarios:

Scenario 1: prediction using single human input

Scenario 2: ML using human behavioral traces + TUS features

Scenario 3: Aggregated crowd signals

Scenario 4: Human + TUS hybrid feature space

Scripts for these scenarios are in the code/ directory.

## 3. Getting Started
Requirements

This project uses Python 3.x.
Each subdirectory README provides its own exact requirements.

To prepare an environment:
pip install -r requirements.txt


## 4. Benchmark Data

The data/ directory contains:

Raw and processed human response exports

Merged data versions used in ML experiments

Feature-engineered tables

TUS method outputs

Metadata describing tables, schemas, and survey versions

A detailed description of each file is available in data/README.md.


## 5. Results and Analysis

The results/ directory stores:

Accuracy, precision, recall, F1, calibration, and resolution metrics

LOVO (leave-one-version-out) cross-validation tables

Scenario comparisons

Ablation study outputs

Additional exploratory analyses

A full explanation of each results file appears in results/README.md.

## 6. Technical Report

Technical_Report.pdf contains extended documentation, including:

Benchmark design

Human behavior profiling

Analysis of disagreement and ambiguity

TUS method benchmarking

ML and LLM setup and ablations

Calibration insights

Prompt Engineering Strategies 
